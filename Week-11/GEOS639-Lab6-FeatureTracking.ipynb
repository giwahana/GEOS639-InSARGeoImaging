{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9270f7-0a1f-4426-939b-353119a07fc5",
   "metadata": {},
   "source": [
    "<img src=\"Figs/Banner.JPG\" width=\"100%\" />\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 639 Geodetic Imaging </b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 6: Glacier Velocity Mapping using Template Matching </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer; University of Alaska Fairbanks </b> <br>\n",
    "\n",
    "</font>\n",
    "\n",
    "<img style=\"padding: 0px 0px 0px 10px\" src=\"Figs/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /> <font size=\"3\"> This lab will let you exercise template matching techniques for the application of glacier velocity mapping. We will initially perform template matching using cross-correlation techniques on a pair of Sentinel-2 optical images covering an are of south east Alaska that includes Malaspina and Hubbard Glaciers. Subsequently, we will work with a large number of Sentinel-1 image pairs processed using the AutoRIFT algorithm covering the same area. This larger set of images will help understand recent changes in glacier velocity at these glaciers. \n",
    "    \n",
    "The notebook will discuss error sources of template matching techniques near the end of the workflow.\n",
    "\n",
    "<b>This Lab is part of the UAF course GEOS639 Geodetic Imaging. It will introduce the following data analysis concepts:</b>\n",
    "\n",
    "- Using template matching for the application of glacier velocity mapping\n",
    "- How to order template matching-based offset maps using the AutoRIFT algorithm \n",
    "- Analysis of glacier velocity changes at Maimport url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)laspina glacier, Alaska.\n",
    "</font>\n",
    "<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d1009-ad16-4cfe-9d99-1b0fefd4ef31",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"4\"> <b>THIS NOTEBOOK DOES NOT INCLUDE A HOMEWORK ASSIGNMENTS.</b></font> \n",
    "<br> \n",
    "<font size=\"3\"> This Lab is allowing you to exercise template-based tracking technology and gets you exposed to some of the datasets coming out of template matching workflows. There is no homework assignment attached to this lab.</font> <br>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398694a8-8672-4811-867d-5701eeb3a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda6ad4-b367-4214-83d0-aa248dca8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "import netCDF4 as nc\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/unavco':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"unavco\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select \"unavco\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"unavco\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cabde-e541-4017-93da-98b70ff8c44d",
   "metadata": {},
   "source": [
    "# 0. Importing Relevant Python Packages and Cloning Necessary GitHub Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65a36f-4889-4798-8d41-85c8991ca233",
   "metadata": {},
   "source": [
    "First step in any notebook is to import the required Python libraries into the Jupyter environment. In this notebooks we use the following scientific libraries:\n",
    "<ol type=\"1\">\n",
    "    <li> <b><a href=\"http://www.numpy.org/\" target=\"_blank\">NumPy</a></b> is one of the principal packages for scientific applications of Python. It is intended for processing large multidimensional arrays. </li>\n",
    "    <li> <b><a href=\"https://matplotlib.org/index.html\" target=\"_blank\">Matplotlib</a></b> is a low-level library for creating two-dimensional diagrams and graphs. With its help, you can build diverse charts, from histograms and scatterplots to non-Cartesian coordinates graphs. </li>\n",
    "</ol>\n",
    "<br>\n",
    "In addition to that, we will also <b>import clone a <a href=\"https://github.com/markf6/pycorr_iceflow\" target=\"_blank\">GitHub repository</a></b> generated by Mark Fahnestock, UAF that includes glacier tracking code that is using cross-correlation analysis implemented in a rather lightweight openCV framework</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebe348-616d-4ed7-9d4b-0ad820cc1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge fiona --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40aa0f-95a2-4862-9eec-ad2338193e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import opensarlab_lib as asfn\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b2683-05b8-4a4b-9d6f-9476ef4f9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/markf6/pycorr_iceflow.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55849d36-3cb5-4cb3-816c-722f25c814ef",
   "metadata": {},
   "source": [
    "# 1. Introduction to the Workflow We Will Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc8830-6a70-44b6-9310-dff7437f3c84",
   "metadata": {},
   "source": [
    "## 1.1 The Basic Template Matching Approach Used Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68649c20-91b5-47c3-872f-73c378261190",
   "metadata": {},
   "source": [
    "```pycorr_iceflow_1.1.py``` is based on the approach used in the production code of the <b><a href=\"https://nsidc.org/support/how/golive-map-application-user-guide\" target=\"_blank\">GoLIVE Landsat 8 processing</a></b> at NSIDC. It determines offsets at a user-specified grid spacing by comparing a square patch of pixels (a \"chip\") from an earlier image to the pixels in a larger square patch in a later image using the openCV ```cv2.matchTemplate``` function, a dft-based <b>cross correlation method</b> (see figure below) returning a correlation surface at integer pixel offsets between the two image chips. Sub-pixel offsets are determined by finding the peak of the spline of the correlation surface in the vicinity of the highest integer correlation peak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1bd07f-1872-476a-9cf4-3ed0bb651277",
   "metadata": {},
   "source": [
    "## 1.2 Benefits of the PyCorr Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb315d-48bf-410a-96fa-6abfc9aba75b",
   "metadata": {},
   "source": [
    "<img style=\"padding: 0px 0px 0px 10px\" src=\"Figs/TemplateMatching.JPG\" width=\"500\" align=\"right\" />```pycorr``` is a \"relatively\" light weight python script that exploits <b><a href=\"https://gdal.org/\" target=\"_blank\">GDAL</a></b> and <b><a href=\"https://opencv.org/\" target=\"_blank\">openCV</a></b> to rapidly determine offsets in an image pair. Because it uses GDAL for image I/O, it can use image pairs in many geospatial formats, with the caveat that the images do overlap some spatially and that their image pixels have the same size. \n",
    "\n",
    "pycorr produces a netCDF4 file with offsets and correlation values at a user-specified grid resolution in the same projection as the original images if the input images are in a UTM or Antarctic Polar Stereo (epsg:3031) projection - this is the set of projections used for Landsat imagery. If your images are in a a different projection, you are not out of luck - use the -```output_geotiffs_instead_of_netCDF``` option to output in the same projection as the input images - this option allows any projection GDAL knows about, which is most. The issue here is that the netCDF4 cf geolocation spec requires a variable block in the output file that is named after the projection, making it difficult to support all projections in a simple way.\n",
    "\n",
    "There are a number of packages that provide similar analyses and may have more sophisticated approaches to identifying and filtering \"noisy\" matches, which can be due to cloud cover, surface change, low contrast features or an absence of features, shadows, and low signal-to-noise input imagery. ```pycorr``` is intentionally simple - it does not use a series of larger chip sizes if the initial match fails to find a peak at a location; it returns a limited set of metrics that help document the uniqueness and strength of a peak that can be used to filter the output, but it does not attempt to provide an error estimate for each match.\n",
    "\n",
    "```pycorr``` is computationally fast because of the use of numpy and the openCV library, and so can process an image pair in minutes or tens of minutes depending on the image sizes and requested grid spacing and maximum offset specified for the search. This process can be sped up by using land and ocean masks to limit search distances off of the ice and also by using reference ice flow speed maps to set search distances larger in fast flowing areas and smaller in slow flowing areas, but for simplest uses these are not applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6d6a2-2356-4614-bfd0-a49686dab329",
   "metadata": {},
   "source": [
    "# 2. Exercising PyCorr on a Sentinel-2 Image Pair over Malaspina Glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ddc04-30fe-4e42-8175-2899f611d567",
   "metadata": {},
   "source": [
    "## 2.1 Accessing the Sentinel-2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9d43e-7445-498a-83c5-6b1d851b65d7",
   "metadata": {},
   "source": [
    "We will use a pair of Sentinel-2 images acquired in May 2020 to demonstrate how ```pycorr``` works and which parameter it needs for creating an offset field. You can replicate this run by replacing the image pairs used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71623bcc-84b5-440a-b701-d2354e0828a0",
   "metadata": {},
   "source": [
    "We first get two Sentinel 2 images from the <b><a href=\"https://registry.opendata.aws/sentinel-2-l2a-cogs/\" target=\"_blank\">AWS S2 Level2A CloudOptimizedGeotiff (COG) public data archive</a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d8e95-fab1-4d5c-b700-ccb80925b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/7/V/EG/2020/5/S2B_7VEG_20200512_0_L2A/B08.tif --output S2B_7VEG_20200512_0_L2A_B08.tif\n",
    "!curl https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/7/V/EG/2020/5/S2A_7VEG_20200527_0_L2A/B08.tif --output S2A_7VEG_20200527_0_L2A_B08.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887028e3-47e1-490c-9a75-82767635638f",
   "metadata": {},
   "source": [
    "<img style=\"padding: 0px 0px 0px 10px\" src=\"Figs/Malaspina-Hubbard.jpg\" width=\"500\" align=\"right\" />This image pair we will be analyzing in this part of the notebook covers a part of south-east Alaska including Malaspina and Hubbard glacier. \n",
    "\n",
    "<b>Malaspina glacier</b> is the largest piedmont glacier in the world. Situated at the head of the Alaska Panhandle, it is about 65 km (40 mi) wide and 45 km (28 mi) long, with an area of some 3,900 $km^2$ (1,500 $mi^2$). It is named in honor of Alessandro Malaspina, a Tuscan explorer in the service of the Spanish Navy, who visited the region in 1791. Malaspina has been undergoing a surge that started in around May 2020 and seems to have ended near the end of 2022. \n",
    "\n",
    "<b>Hubbard Glacier</b> is a glacier located in <b><a href=\"https://en.wikipedia.org/wiki/Wrangell%E2%80%93St._Elias_National_Park_and_Preserve\" target=\"_blank\">Wrangell–St. Elias National Park and Preserve</a></b> in eastern Alaska and Kluane National Park and Reserve in Yukon, Canada, and named after Gardiner Hubbard. The longest source for Hubbard Glacier originates 122 kilometers (76 mi) from its snout and is located at about 61°00′N 140°09′W, approximately 8 kilometers (5 mi) west of Mount Walsh with an elevation around 11,000 feet (3,400 m). A shorter tributary glacier begins at the easternmost summit on the Mount Logan ridge at about 18,300 feet (5,600 m) at about 60°35′0″N 140°22′40″W.\n",
    "\n",
    "Before it reaches the sea, Hubbard is joined by the Valerie Glacier to the west, which, through forward surges of its own ice, has contributed to the advance of the ice flow that experts believe will eventually dam the Russell Fjord from Disenchantment Bay waters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18766-a330-441b-b05e-51e58678a57b",
   "metadata": {},
   "source": [
    "## 2.2 Running ```PyCorr``` to Perform Template Matching-based Glacier Velocity Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cea487-6e62-4688-81f7-f91592f57321",
   "metadata": {},
   "source": [
    "We now will <b>run the ```pycorr``` algorithm</b> to perform template matching on the Sentinel-2 image pair. You'll see some warnings pop up early one. Don't mind those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002260a3-d71a-4845-bb2e-d77fb0ae2209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./pycorr_iceflow/pycorr_iceflow_v1.1.py -imgdir . S2B_7VEG_20200512_0_L2A_B08.tif S2A_7VEG_20200527_0_L2A_B08.tif \\\n",
    "                              -img1datestr 20200512 -img2datestr 20200527 -datestrfmt \"%Y%m%d\" \\\n",
    "                              -inc 10 -half_source_chip 10  -half_target_chip 55 -plotvmax 25 -log10 \\\n",
    "                              -out_name_base S2A_S2BA_7VEG_15_20200512_20200527 -progupdates -use_itslive_land_mask_from_web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc700ff1-9fb5-4373-a990-e80ee6cbaf41",
   "metadata": {},
   "source": [
    "The code worked through the image pair and calculated thousands of offset points. It saved the output as a netCDF4 file, which we will look at in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e156e96-c517-4592-801e-c8a56f39ad1b",
   "metadata": {},
   "source": [
    "## 2.3 Visualizing ```PyCorr``` Template Matching Result for Our Area of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42296848-a690-4886-979d-ac8da6530318",
   "metadata": {},
   "source": [
    "We can now visualize the ```pycorr``` result to evaluate the performance of the algorithm. \n",
    "\n",
    "First we <b>import the netCDF4 file</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06de30b-cbea-46f4-bb0e-fbbc654aa929",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nc.Dataset('S2A_S2BA_7VEG_15_20200512_20200527_hp.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d091736-a30d-4ee9-b92a-fc945fd3f0d9",
   "metadata": {},
   "source": [
    "The output netCDF4 file (S2A_S2BA_7VEG_15_20200512_20200527.nc) includes various layers. We will choose the <b>vv_masked</b> layer to get ice flow speed. Alternatively you can read the <b>vx_masked</b> and <b>vy_masked</b> layers to get the vector components of the flow speed in projection x and y. Ice velocities are stored in the unit of <b>meters/day</b>.\n",
    "\n",
    "Read the <b>vv_masked</b> layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c700968-3d04-4958-a5d6-4f9035ee7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_masked = np.ma.masked_where(ds['vv_masked']==0, ds['vv_masked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a683b-387c-4dbb-adad-a0a1e4e7405f",
   "metadata": {},
   "source": [
    "We will also <b>read the UTM coordinate information</b> out of the netCDF4 file for plotting purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980f2db-db71-4d2f-9686-77d69192d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTMx = ds.variables['x'][:]\n",
    "UTMy = ds.variables['y'][:]\n",
    "TM = ds.variables['transverse_mercator'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154db15a-4a94-4709-9abe-35bafe3f5f45",
   "metadata": {},
   "source": [
    "Now we can <b>visualize the velocity information</b> in an interactive plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb956d-6a92-43d4-b350-08c792d69fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Glacier Tracking Results', fontsize=16)\n",
    "dx = (UTMx[1]-UTMx[0])/2.\n",
    "dy = (UTMy[1]-UTMy[0])/2.\n",
    "extent = [(UTMx[0]-dx)/1000.0, (UTMx[-1]+dx)/1000.0, (UTMy[0]-dy)/1000.0, (UTMy[-1]+dy)/1000.0]\n",
    "plt.imshow(vv_masked*365.0, extent=extent, cmap='jet', vmin = 0, vmax = 3000)\n",
    "plt.ylabel('UTM North [km]', fontsize=14)\n",
    "plt.xlabel('UTM East [km]', fontsize=14)\n",
    "plt.grid()\n",
    "cbar = plt.colorbar()\n",
    "#legend\n",
    "cbar.set_label('Velocity in m/yr', rotation=270, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2dcd07-e843-4b1b-9c49-67e9768a754e",
   "metadata": {},
   "source": [
    "# 3. Stack of Velocity Maps over Malaspina Glacier Generated using AutoRIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815e7d0-8a18-49ee-96b6-5366af80afc0",
   "metadata": {},
   "source": [
    "We have seen in the previous example how effective template matching can be for the application of glacier velocity mapping. BUT, we've also seen that the process of template matching is time consuming. Therefore, <b>offloading the process of ice velocities mapping to an automatic service</b> is appealing, especially when one is interested in mapping glacier motion across a longer time span. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68756d-a75d-4b0f-baca-bca59db16285",
   "metadata": {},
   "source": [
    "The Alaska Satellite Facility has recently made the template-tracking based AutoRIFT processor available through its search interface Vertex. To order AutoRIFT data, follow the following general workflow:\n",
    "<ul>\n",
    "  <li>Go to Vertex at <b><a href=\"https://search.asf.alaska.edu/\" target=\"_blank\">https://search.asf.alaska.edu/</a></b></li>\n",
    "  <li>Search for Sentinel-1 Single Look Complex (SLC) images over your area of interest</li>\n",
    "  <li>Use the SBAS search to create image pairs for AutoRIFT processing.</li>\n",
    "  <li>Click on the \"custom processing icon (three nested squares) to submit your jobs</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b3786-19e0-465d-9130-1cda7bc4879c",
   "metadata": {},
   "source": [
    "## 3.1 Retrieving AutoRIFT Stack over Malaspina Glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740633a-805a-4a2f-84fd-6b8ff38d04a6",
   "metadata": {},
   "source": [
    "Using the approach described above, I have ordered 1-year worth of AutoRIFT velocity maps from <b><a href=\"https://search.asf.alaska.edu/\" target=\"_blank\">ASF</a></b>. In total, I have collected 70 glacier velocity estimates over this time period, using 12-day and 24-day Sentinel-1 SAR pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87a799-5444-4d39-b884-0a23c31ae506",
   "metadata": {},
   "source": [
    "I have prepared these data sets and deposited them for you in an AWS S3 storage bucket. In a first step, we will <b>download these data and unzip them</b> into a folder called ```lab_6_data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77db57b-84ce-4715-892c-8c74401b3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target folder\n",
    "path = f\"{os.getcwd()}/lab_6_data\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "os.chdir(path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d024816-f31b-4ab8-a806-55e1ce11c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfering data from Amazon AWS S3\n",
    "malaspina_path = 's3://asf-jupyter-data-west/MalaspinaAutoRIFT.zip'\n",
    "!aws --region=us-west-2 --no-sign-request s3 cp s3://asf-jupyter-data-west/MalaspinaAutoRIFT.zip MalaspinaAutoRIFT.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56af2b-382a-4a79-bae2-0b53038ce6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping data\n",
    "mala_path = f\"{path}/MalaspinaAutoRIFT.zip\"\n",
    "asfn.asf_unzip(str(path), str(mala_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc39f0d-2b5a-4431-a748-730e679d7bcc",
   "metadata": {},
   "source": [
    "## 3.2 Create VRT And Explore Data Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f97db-69f6-4432-8742-b7c60a3055b3",
   "metadata": {},
   "source": [
    "We first <b>define two python functions</b> to identify all of our ```AutoRIFT``` Tiff files and to extract the image dates for these files. Because I am lazy, I am using the date of the reference image here. A more sophisticated approach could be to use the mean date between reference and secondary image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bccae-a9a9-4423-8988-c33ff4ad3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_paths(paths):\n",
    "    tiff_paths = !ls $paths | sort -t_ -k5,5\n",
    "    return tiff_paths\n",
    "\n",
    "def get_dates(pths):\n",
    "    dates = []\n",
    "    for p in pths:\n",
    "        for name_chunk in p.split('/')[-1].split('_'):\n",
    "            nums = list(range(48, 58))\n",
    "            if len(name_chunk) == 15 and ord(name_chunk[0]) in nums: \n",
    "                date = name_chunk.split('T')[0]\n",
    "                dates.append(date)\n",
    "                break\n",
    "    dates.sort()\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946c853-c682-4d39-90cf-9564252ce140",
   "metadata": {},
   "source": [
    "Now we <b>use these functions</b> to identify our GeoTIFFs and extract associated image dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e90b3d-ff5f-40fc-b58d-478016e20fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = \"*.tif*\"\n",
    "tiff_paths = get_tiff_paths(paths)\n",
    "\n",
    "dates=get_dates(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc167a2-e679-491e-a57a-eb6197a00e9c",
   "metadata": {},
   "source": [
    "Now we <b>create a virtual raster table (VRT)</b> holding the information about all our GeoTIFFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb278a6e-2e95-438e-8096-21f11bf75207",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = 'raster_stack.vrt'\n",
    "!gdalbuildvrt -separate $raster_path $paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c4c71-62ea-46ab-9fd0-734275f7dabf",
   "metadata": {},
   "source": [
    "We also <b>create a Pandas time index</b> fpr our data and print the image dates. You see we have 70 image pairs in our stack with reference image times between Jan 09, 2021 and March 29, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4181c4d-b4ea-484b-b696-365ac49b5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pd.DatetimeIndex(dates)\n",
    "\n",
    "for jacqdate, acqdate in enumerate(time_index):\n",
    "    print('{:4d} {}'.format(jacqdate, acqdate.date()),end=' ')\n",
    "    if (jacqdate % 5 == 4): print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff191e-4544-4320-85bc-b8e9d01b69d7",
   "metadata": {},
   "source": [
    "Now we <b>read the data</b> and mask out values associated with bad matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cd827-94cf-4523-be61-6d2032cc5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = gdal.Open(raster_path)\n",
    "rasterstack = img.ReadAsArray()\n",
    "rasterstack_masked = np.ma.masked_where(rasterstack<0, rasterstack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a36c0-93a5-4a44-b2c6-1d172f2f50b0",
   "metadata": {},
   "source": [
    "## 3.3 Visualize Mean Glacier Velocity and Enable Extracting of Pixel-by-Pixel Velocity Time Series Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ccac32-40bd-4374-8681-222f2535cff5",
   "metadata": {},
   "source": [
    "We will now visualize an average velocity image in a way that we can move our mouse over the image and visualize the line/sample image coordinates. This will help us create time-series information for the most interesting image locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5747a4-f6b9-4ee9-841a-3be704030836",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_mean = np.nanmean(rasterstack_masked, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ca3bb-bbff-4255-aa8d-170dccec49cf",
   "metadata": {},
   "source": [
    "To do so, we first **create some helper functions:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62e405-1709-4511-bb14-d2f79894529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class pixelPicker:\n",
    "    def __init__(self, image, width, height):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.fig = plt.figure(figsize=(width, height))\n",
    "        self.ax = self.fig.add_subplot(111, visible=False)\n",
    "        self.rect = patches.Rectangle(\n",
    "            (0.0, 0.0), width, height, \n",
    "            fill=False, clip_on=False, visible=False\n",
    "        )\n",
    "       \n",
    "        self.rect_patch = self.ax.add_patch(self.rect)\n",
    "        self.cid = self.rect_patch.figure.canvas.mpl_connect('button_press_event', \n",
    "                                                             self)\n",
    "        self.image = image\n",
    "        self.plot = self.gray_plot(self.image, fig=self.fig, return_ax=True)\n",
    "        self.plot.set_title('Select a Point of Interest')\n",
    "        \n",
    "        \n",
    "    def gray_plot(self, image, vmin=None, vmax=None, fig=None, return_ax=False):\n",
    "        '''\n",
    "        Plots an image in grayscale.\n",
    "        Parameters:\n",
    "        - image: 2D array of raster values\n",
    "        - vmin: Minimum value for colormap\n",
    "        - vmax: Maximum value for colormap\n",
    "        - return_ax: Option to return plot axis\n",
    "        '''\n",
    "        if vmin is None:\n",
    "            vmin = np.nanpercentile(self.image, 10)\n",
    "        if vmax is None:\n",
    "            vmax = np.nanpercentile(self.image, 99)\n",
    "        if fig is None:\n",
    "           my_fig = plt.figure() \n",
    "        ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "        pos = ax.imshow(image, cmap= 'Blues' , interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "        cbar = fig.colorbar(pos, ax=ax)\n",
    "        cbar.set_label('Velocity in m/yr', rotation=270)\n",
    "        if return_ax:\n",
    "            return(ax)\n",
    "        \n",
    "    \n",
    "    def __call__(self, event):\n",
    "        print('click', event)\n",
    "        self.x = event.xdata\n",
    "        self.y = event.ydata\n",
    "        for pnt in self.plot.get_lines():\n",
    "            pnt.remove()\n",
    "        plt.plot(self.x, self.y, 'ro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b15192-68b9-4180-a708-0daa364e3d2e",
   "metadata": {},
   "source": [
    "Now we are ready to plot the average glacier velocity image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a3252-a1e6-4d2f-b66c-d8a9da28692f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"4\"> <b>Click into the figure to select a point interest for which you want to analyze the glacier velocity time series.</b></font> \n",
    "<br> \n",
    "<font size=\"3\"><b>Note</b>: This data set is in polar-stereo projection, giving it the rotated appearance.</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773ff59-c2ac-4234-9dc1-47d83c1505bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_xsize = 7.5\n",
    "fig_ysize = 7.5\n",
    "my_plot = pixelPicker(temporal_mean, fig_xsize, fig_ysize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621abd26-c5f9-4eb0-8ff1-63ae5dc888d8",
   "metadata": {},
   "source": [
    "**Save the selected coordinates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0ee8f-c74b-49a9-955d-f77ebe187077",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarloc = (ceil(my_plot.x), ceil(my_plot.y))\n",
    "print(sarloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a89539-b450-49fd-a9e3-4e30fab7f1d4",
   "metadata": {},
   "source": [
    "## 3.4 Plot Glacier Velocity Time Series at Point Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1abeb81-def3-4c15-9c0b-c7f43cee2a09",
   "metadata": {},
   "source": [
    "You've picked a location of interest above. Now, let's **pick a ```[3x3]```-sized rectangle around a center pixel which we selected ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c7d35-1f9d-46e6-bbbf-d10f3ae5523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = (3, 3) # choose a 3 by 3 rectangle\n",
    "bands = img.RasterCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62246ddb-9784-4f1d-beda-f668e04b7535",
   "metadata": {},
   "source": [
    "**... and extract the time series for this small area around the selected center pixel in a memory-efficient way (needed for larger stacks):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd6fd9-6130-4abe-b909-3330d8dd7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "bs_aggregated = []\n",
    "for band in range(bands):\n",
    "    rs = img.GetRasterBand(band+1).ReadAsArray(sarloc[0], sarloc[1], \n",
    "                                               extent[0], extent[1])\n",
    "    rs_masked = np.ma.masked_where(rs<0, rs)\n",
    "    rs_mean = np.nanmean(rs_masked)\n",
    "    bs_aggregated.append(rs_mean)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "plt.title('Glacier Velocity Results', fontsize=16)\n",
    "ax.scatter(time_index, bs_aggregated, color='k', marker='o')\n",
    "ax.set_xlabel('Date', fontsize=14)\n",
    "ax.set_ylabel('Glacier Velocity [m/day]', fontsize=14)\n",
    "plt.xticks(rotation = 45)\n",
    "ax.set_ylim([0, 4500])\n",
    "plt.grid()\n",
    "fig.tight_layout() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc600fb-d445-4ecd-a128-725002bc4a63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"4\"> <b>Please Explore Several Points on the Map by going back to the average velocity figure, selecting a different point, and running the code cells under the average velocity figure one more time.</b></font> \n",
    "<br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83b219-8cda-45ad-85de-e6e1c8a696cd",
   "metadata": {},
   "source": [
    "# 4 Version Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4e760-52ff-4e26-a001-1a4d5ecc6701",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 639 Geodetic Imaging - Version 1.0.0 -  April 2022 \n",
    "    <br>\n",
    "        <b>Version Changes:</b>\n",
    "    <ul>\n",
    "        <li>First version of this lab</li>\n",
    "    </ul>\n",
    "    </i>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unavco [conda env:.local-unavco]",
   "language": "python",
   "name": "conda-env-.local-unavco-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
